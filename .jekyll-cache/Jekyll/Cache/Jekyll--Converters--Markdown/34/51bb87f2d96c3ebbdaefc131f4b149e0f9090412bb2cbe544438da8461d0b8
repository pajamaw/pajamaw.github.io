I"™6<p>Big O notation can be <em>different</em> to those unfamiliar with the concept. So to beginÂ hereâ€™s a little road map of a few hot topics that I find frequently when the subject is spoken about.</p>

<ol>
  <li>
    <p>What is Big O (and which one are we talking about?)</p>
  </li>
  <li>
    <p>Why Big O?</p>
  </li>
  <li>
    <p>Examples of Big O in the Not-So Wild: Datastructures and Sorts</p>
  </li>
</ol>

<h2 id="what-isbig-o">What isÂ Big O?</h2>

<p>Big O describes the efficiency of an algorithm. SpecificallyÂ itâ€™s usedÂ to describe both the efficiency in time (Big O Time) and memory use (Big O Space) of an algorithm. If youâ€™re non-technical, know that an algorithm is just a process of operations designed to solve a problem.</p>

<p>Sorting through an array? You just usedÂ an algorithm!</p>

<p>Selecting an instance from a database? Algorithm!</p>

<p>Finding the middle point for you and a friend to meet? Algoooo</p>

<p>Now there are three different types of Big O used in academia (Big O, Big Omega, and Big Theta) to describe runtime. Big O corresponds to the upper bound in a graph, Big Omega the lower bound, and Big Theta the tight bound.</p>

<p>These concepts shouldnâ€™t be confused with best, worst, and expected case scenarios; each O notation can be used to describe each scenario. That being said, the Big notation used inÂ programming, Big O (which in academia is closer to Big Theta) i.e. the tight bound for a function will be focused on.</p>

<p>Big O describes the rate of increase (in memory i.e. space or time) as â€œnâ€, your input, goes up. If your â€œnâ€ is equal to an array, and you have to iterate through that array, youâ€™re going to do that in Big O(n) time, where n is the number of elements in the input array. If you have an array that youâ€™reÂ grabbing anÂ element from a specific index, that will take Big O(1) time, as it is done in 1 step no matter how large the array is.</p>

<p>Say we needed to run through an array twice,Â Â does it take Big O(2n) time? No, weâ€™d still say that it takes Big O(n) time. When we describe Big O we donâ€™t use constants. A function that runs in Big O(n) we say will equal a function that runs in Big O(2n) because for different values of â€œnâ€ those can be equal. So never use constants when youâ€™re describing a runtime!</p>

<h2 id="why-big-o">Why Big O?</h2>

<p>We use Big O to find the time and memoryÂ that will be required toÂ run a function. But what does that really mean and why care?</p>

<p>Well letâ€™s say that you just got a job with a dating companyÂ with 200Â members and this jobÂ paysÂ you to pick matches among those people. 100 guys and 100 girls. Profiles are in an array and each user can match with either sex.Â Well letâ€™s say that the company doesnâ€™t allow youÂ to skip users, and users donâ€™t get removed from the dating pool when theyâ€™re matched with someone else.</p>

<p>So you begin manually. Youâ€™re presented with one personâ€™s picture and then you go through each of the 200 (including the same personâ€™sÂ picture) to create matches. Until you go through each potential partner, then you start on the second partnerâ€¦ad nauseum. It takes about 5 seconds per picture and since you donâ€™t get your check until you finish, it takes 55.55 hours!Â Thatâ€™s because the run time for the function is Big O(n^2). ForÂ each person on the site, you have to go through the every individual again.</p>

<p>Whatâ€™s more efficent?Â Just about anything. You could seek to limit the number of users that an individual getâ€™s access to by grouping and matching based upon a set of preferences. Or stop once a user has a certain number of matches.</p>

<p>But what about Big O space? Well letâ€™s use those that examples again. In the situation youâ€™ll have to use some sort of data structure to store your matches for a person. Letâ€™s say you use another array. That means that each person will need to have their own  for their matches. That means youâ€™re about to build out 200 arrayâ€™s of up to 200 people eachâ€¦now that doesnâ€™t take much time to add to as pushing to an array takes Big O(1) time. But now you have n users each with an n sized array thatâ€™s n^2 spaced. The problems you face when you start to consider the implications of algorithms speed and space constraints will often be give and take. Itâ€™s always about finding the solution with the best pro to con ratio for your situation.</p>

<h2 id="examples-of-big-o-in-the-not-so-wild-datastructures-and-sorts">Examples of Big O in the Not-So Wild: Datastructures and Sorts</h2>

<p>Big O comes in the most handy when thinking about data structures and sorting algorithms. Weâ€™re going to be going over these in the next several posts in this series, but for now, weâ€™ll just begin by describing them and their various methods and runtimes.</p>

<p>FirstÂ <img src="https://rubydoobiedoo.files.wordpress.com/2016/09/region-capture-2.png" alt="Region capture 2.png" /></p>

<h3 id="datastructures-">Datastructures :</h3>

<p><strong>Array:Â </strong> The array is a foundational data structure. Itâ€™s unique capability that makes it invaluable, is itâ€™s O(1) time for access and insertion. If you know the position of an element, you can immediately get access to it. Arrayâ€™s in many languages are often used to create other datastructures like stacks, queues, and hash tables.</p>

<p><strong>Stack:Â </strong>A stack is a datastructure that is peculiar in the fact that it works like a LIFO, Last in First Out, array. The latest addition to the stack, would be accessed first, while the others can only be accessed by popping out the previous item one before itself. Think of stacks like the toddler game the tower of babel. Generally Stacks will use functions like push(item), pop(), peek(), and search(item). These methods take O(1), O(1), O(1), and O(n) time respectively. Stacks can be used to implement queues and are often used in DFS algorithms.</p>

<p><strong>Queue:Â </strong>A queue is a datastructure that is peculiar in the fact that it is a FIFO, First in First Out, array. Meaning that the oldest item in the queue, is the firstÂ outÂ of the Queue. Like a Stack, QueueÂ can only be searched by going through each element. Think about a QueueÂ like you would any line youâ€™ve ever been in. Queues generally will use functions like unshift(), push(item), peek() (to view the bottom element), and search(item). These methods take O(1), O(1), O(1), and O(n) time respectively. Queues are often used in BFS algorithms.</p>

<p><strong>Singly-Linked List:Â </strong>A Singly-Linked List (SLL)Â is a datastructure that is peculiar in the fact that each node is connected only to the next node, with the exception of the last node. the the oldest addition to the SLL, the Head/root, is only one that a user has direct accessiblity to, much like a queue. SSLâ€™sÂ can only be searched by going through each nodeâ€™sÂ â€™nextâ€™ attribute that points to the following node. In a similar way to queue, adding and removing from/to the beginning of a SSL can be done in O(1) time, while adding/removing/searching to any other pointÂ would take O(n) time.</p>

<p><strong>Hash Table:Â </strong>A Hash TableÂ is a datastructure that is peculiar in the fact that itâ€™s essentially an array with a plan. It contains buckets filled with key-value pairs that allows you to access any element in O(1) time and add or delete any item in O(1) time. A hash is stores its key value pairs via a hashing function that uses the input items key to create a hash and via another algorithm will calculate an index that points to a particular bucket in the array. Hash tables can be remarkably efficient with the correct hashing implementation. Hashes are a critical  datastructures thatâ€™s also often used in conjunction with other more advanced datastructures.</p>

<p><strong>Trees:Â </strong>A treeÂ is a  datastructure, that are similar to SSLâ€™s in the way that theyâ€™re made up of nodes that connect to other nodes. Unlike SSLâ€™s though, these nodes have links to both their parent and children nodes. Nodes that donâ€™t have children are called leaf nodes. TheÂ firstÂ node is called the root. There are several different sorts of Trees, from trees that only accept 2 children (Binary trees and Binary Search Trees), to some that only offer a single character of information (Tries). Treeâ€™s are used to connect points of information that relate to one another, and can be used with a variety of data structures. Sorted trees often have some of the best Big O search capabilities at O(n log(n)) as they can follow branches based off of simple conditionals.</p>

<h3 id="sorting-algorithms">Sorting Algorithms:</h3>

<p><strong>Quick Sort: Â </strong>The Quick Sort algorithm works by finding a pivot point, and then sorting the two resulting arrays around said pivot point. The act of finding this pivot point will generally sort the array into a higher side, and a lower side. Depending on the size of array, this pivot point will be foundÂ recursively on each minor array. The act of finding this pivot point is an algorithm in itself, and several different of these have made their way into modern use with the Hoare partition being the most efficient. With sorts you do have to worry about the average AND worst case scenario of time it will take, as it can be something to worry about because who knows how your data will be positioned? Itâ€™s average time is O(n log(n)), but it can be as bad as O(n^2) in the right conditions.</p>

<p><a href="https://commons.wikimedia.org/wiki/File:Sorting_quicksort_anim.gif#/media/File:Sorting_quicksort_anim.gif"><img src="https://upload.wikimedia.org/wikipedia/commons/6/6a/Sorting_quicksort_anim.gif" alt="Animated visualization of the quicksort algorithm. The horizontal lines are pivot values." /></a>
By <a href="https://en.wikipedia.org/wiki/User:RolandH">en:User:RolandH</a>, <a href="http://creativecommons.org/licenses/by-sa/3.0/">CC BY-SA 3.0</a>, <a href="https://commons.wikimedia.org/w/index.php?curid=1965827">https://commons.wikimedia.org/w/index.php?curid=1965827</a></p>

<p><strong>Merge Sort:Â </strong>The merge sort works on the â€œDivide and Conquerâ€ theory. It divides lists into sublists, then sorts those sublists, it then combines, and repeats the division asÂ necessary. The final sublist becomes the ultimate sorted list. ItÂ Â is a remarkably resourceful sort whoseÂ expectedÂ and worst case sorts are both O(n log(n)).</p>

<p><a href="https://commons.wikimedia.org/wiki/File:Merge-sort-example-300px.gif#/media/File:Merge-sort-example-300px.gif"><img src="https://upload.wikimedia.org/wikipedia/commons/c/cc/Merge-sort-example-300px.gif" alt="Merge-sort-example-300px.gif" /></a>
By <a href="Swfung8&amp;action=edit&amp;redlink=1">Swfung8</a> - Own work, <a href="http://creativecommons.org/licenses/by-sa/3.0">CC BY-SA 3.0</a>, <a href="https://commons.wikimedia.org/w/index.php?curid=14961648">https://commons.wikimedia.org/w/index.php?curid=14961648</a></p>

<p><strong>Bubble Sort:Â </strong>The bubble sort works by â€œbubbling upâ€ higherÂ values with lower values. Itâ€™s a commonly used sorting tactic for trees and lists, where you can only access one value at a time. In order to do so, you must iterate through the array potentially n^2 times, making it a very time costly sort to do, with an average of O(n^2) time.</p>

<p><a href="https://commons.wikimedia.org/wiki/File:Bubble-sort-example-300px.gif#/media/File:Bubble-sort-example-300px.gif"><img src="https://upload.wikimedia.org/wikipedia/commons/c/c8/Bubble-sort-example-300px.gif" alt="Bubble-sort-example-300px.gif" /></a>
By <a href="Swfung8&amp;action=edit&amp;redlink=1">Swfung8</a> - Own work, <a href="http://creativecommons.org/licenses/by-sa/3.0">CC BY-SA 3.0</a>, <a href="https://commons.wikimedia.org/w/index.php?curid=14953478">https://commons.wikimedia.org/w/index.php?curid=14953478</a></p>

<p><strong>Counting Sort:Â </strong>The counting sort is personally my favorite sort. Why? I think itâ€™s absolutely brilliant. Unfortunately it does require threeÂ bits of knowledge for your array (Iâ€™m sure you could hack around it but for the common implementation these are required)Â that it only has positive values, that each value is an integer, and that you know the max. The graphic below shows it (decently), but it relies on you creating an array from the size of the max value. You then iterate through the firstÂ array, and every time you hit add 1 to the index of the range array to the index of that number. So if you have an array with 3 1â€™s, Â rangeArray[1] will equal 3! This addition to the rangeArray takesÂ O(1) time due to Arrayâ€™s O(1) access time! You then (can) create a third final array which as you go through each index, you check the rangeArray and add each number the corresponding amount of times that it has in its â€œcounterâ€. I think itâ€™s a remarkably brilliant â€œhackâ€ of the arrayâ€™s O(1) access time and can be done consistently at O(n+k) where n are the number of elements and k is itâ€™s range.</p>

<p><img src="https://rubydoobiedoo.files.wordpress.com/2016/09/counting-sort.gif" alt="counting-sort.gif" /></p>

<p>This may seem like a lot to graspâ€¦and youâ€™re right! If youâ€™re new to programming, youâ€™re most likely new to Computer Science principles which all of these are based upon. But I promise, learning these more and more will help you counter any problem you find, and youâ€™ll be amazed at their utility in analytical problem solving in real life and for fun (check outÂ <a href="http://fivethirtyeight.com/tag/the-riddler/">Nate Silverâ€™s company, 538, one of their users posts great riddles one of which I figured out using a Binary search method just the other day!)</a></p>

<p>This was just a brief (lol 2500 words) intro to these concepts, but Iâ€™ll be going into each more in depth in the posts to come! Stay Tuned.</p>
:ET