I"Î@<p>This is the first (real) post in my Filling in the Blanks series. If youâ€™re like me, you most likely went through a programming bootcamp of some kind in hopes to achieve fame, fortune, a new hobby, and/or a job! If you are I wish you luck in all of your pursuits, and hope this finds you in time for that big interview.</p>

<p>Big O notation can definitely be a scary principle to those unfamiliar with the concept. So to beginÂ hereâ€™s a little road map of a few hot topics that I find frequently when the subject is spoken about.</p>

<ol>
  <li>
    <p>What is Big O (and which one are we talking about?)</p>
  </li>
  <li>
    <p>Why Big O?</p>
  </li>
  <li>
    <p>Examples of Big O in the Not-So Wild: Datastructures and Sorts</p>
  </li>
</ol>

<p>Before we go into though, I just want to add the disclaimer</p>

<h2 id="what-isbig-o">What isÂ Big O?</h2>

<p>Great question(s)!</p>

<p>Big O describes the efficiency of an algorithm. SpecificallyÂ itâ€™s usedÂ to describe both the efficiency in time (Big O Time) and memory use (Big O Space) of an algorithm. Before we go further, remember that an algorithm doesnâ€™tÂ necessarily just connect you to a huge complicated calculus theorem, every method/function thatÂ you create in programming is an algorithm.</p>

<p>Sorting through an array? You just usedÂ an algorithm!</p>

<p>Selecting an instance from a database? Algorithm!</p>

<p>Everything we do in programming is algorithmic, even if you didnâ€™t realize it.</p>

<p>Now there are actually three different types of Big O used in academia (Big O, Big Omega, and Big Theta) to describe runtime. Big O corresponds to the upper bound in a graph, Big Omega the lower bound, and Big Theta the tight bound.</p>

<p>Now donâ€™t get these concepts confused with best, worst, and expected case scenarios, because they donâ€™t actuallyÂ correspond with any in particular each O notation can be used to describe each scenario. That being we will talk about the Big notation used inÂ programming, Big O (which in academia is closer to Big Theta) i.e. the tight bound for a function.</p>

<p>So, back to a generalÂ programming definition, Big O describes the rate of increase (in memory i.e. space or time) as â€œnâ€, your input, goes up. If your â€œnâ€ is equal to an array, and you have to iterate through that array, youâ€™re going to do that in Big O(n) time, where n is the number of elements in the input array. If you have an array that youâ€™reÂ grabbing anÂ element from a specific index, that will take Big O(1) time, as it is done in 1 step no matter how large the array is.</p>

<p>Say we needed to run through an array twice,Â Â does it take Big O(2n) time? No weâ€™d still say that it takes Big O(n) time. When we describe Big O we donâ€™t use constants. A function that runs in Big O(n) we say will equal a function that runs in Big O(2n) because for different values of â€œnâ€ those can be equal. So never use constants when youâ€™re describing a runtime!</p>

<h2 id="why-big-o">Why Big O?</h2>

<p>So as you might be figuring it out now, we use Big O to find the time and memoryÂ that will be required toÂ run a function. But what does that really mean and why should I care?</p>

<p>Well letâ€™s say that you just got a job with a dating companyÂ with 200Â members (no worries Iâ€™m sure itâ€™ll get more successful!) and this jobÂ paysÂ you to pick matches for OTHER people. Itâ€™s got 100 guys and 100 girls. Those profilesÂ are inÂ one array and each user can match with either sex (cause weâ€™re being progressive in this post).Â Well letâ€™s say that the company doesnâ€™t allow youÂ to skip users, and users donâ€™t get removed from the dating pool when theyâ€™re matched with someone else.</p>

<p>Well, you want to get paid, so you begin. It gives you one personâ€™s picture and then you go through each of the 200 (including the same personâ€™sÂ picture) as you get matches. Until you go through each match, then on to the nextâ€¦ad nauseum. You take about 5 seconds per picture because you care (a little) but you donâ€™t get your check until you finish, so you finishâ€¦.55.55 hours later!Â Thatâ€™s because the run time for the function is Big O(n^2). ForÂ each (n) orÂ element of the array, you have to go through the each (n) or element of the array.</p>

<p>Well now youâ€™re thinking well this has to get better if the company was more efficient right?Â 55.55 hour work days are a lot too! So you find a company that has 200 users, but theyâ€™re in two different arrays, a menÂ and womenâ€™sÂ array. It gives you a random genderâ€™sÂ picture and now you have to cycle through each female picking matches until youâ€™re at the end of the girls, and then to the next guy, and again a cycle through each female â€¦ad nauseum. Then once you finish the first gender you do the second gender. Well how long till you get paid? If it takes you 5 second per couple again, it will take you just under 27 hours to do it! Thatâ€™s a significantly better workday!.</p>

<p>Now we would say that this is Big O(2(m<em>w)) where m is the length of the menâ€™s array and w is the length of the womenâ€™s array, and even though weâ€™re going through the same number of users, it takes less than half Â ofÂ the time. Remember though, in a real setting we would describe this as Big O(m</em>w) as we should drop the constants.</p>

<p>But what about Big O space? Well letâ€™s use those 2 examples again. In both companies youâ€™ll have to use some sort of data structure to store your matches for a person. Letâ€™s say you use an array. That means that each person will need to have their own array for their matches. That means youâ€™re about to build out 200 arrayâ€™s of up to 200 people eachâ€¦now that doesnâ€™t take any time to make as pushing to an array takes Big O(1) time. but it will take up (n^2) space. That also means that the second company, if theyâ€™re using the same system will take m<em>w + m</em>w or Big O(m*w) time (as we drop the constant).</p>

<p>There are definitely better ways to store such information, and we havenâ€™t even gotten into utilizing them for matches! But already you can start to see how Big O would be used to describe real situations.</p>

<h2 id="examples-of-big-o-in-the-not-so-wild-datastructures-and-sorts">Examples of Big O in the Not-So Wild: Datastructures and Sorts</h2>

<p>But even then, Big O comes in the most handy when you use them to think about advanced data structures and sorting algorithms. Weâ€™re going to be going over these in the next several posts in this series, but for now, weâ€™ll just begin by describing them and their various methods and runtimes.</p>

<p>FirstÂ <img src="https://rubydoobiedoo.files.wordpress.com/2016/09/region-capture-2.png" alt="Region capture 2.png" /></p>

<p>Wow, you might be saying how do you even make sense of these charts?</p>

<p>One at a time! (And skipping the hard ones :) )</p>

<h3 id="datastructures-">Datastructures :</h3>

<p><strong>Array:Â </strong>As you may have assumed. The array is in fact a datastructure! Despite it being a simple one it does have one very unique capability that makes it invaluable, itâ€™s O(1) time for access. If you know the position of an element, you can immediately get access to it. Arrayâ€™s in many languages are often used as the baseline to create many advanced datastructures like Stacks, Queues, and Hash Tables.</p>

<p><strong>Stack:Â </strong>A stack is an advanced datastructure that is peculiar in the fact that it can is essentially a LIFO, Last in First Out, array. MeaningÂ that the latest addition to the stack, can be accessed first, while the others can only be accessed by going through each one before it. Think of stacks like the toddler game with the different size rings that can be placed on top of the wooden pole mounted on a wooden platform. Generally Stacks will use functions like push(item), pop(), peek() (to view the top element), and search(item). These methods take O(1), O(1), O(1), and O(n) time respectively. Stacks can be used to implement queues and are often used in DFS algorithms.</p>

<p><strong>Queue:Â </strong>A queue is an advanced datastructure that is peculiar in the fact that it is a FIFO, First in First Out, array. Meaning that the leastÂ recent addition to the queue, is the firstÂ outÂ of the Queue. Like a Stack, QueueÂ can only be searched by going through each element. Think about a QueueÂ like you would any line youâ€™ve ever been in. Queues generally will use functions like unshift(), push(item), peek() (to view the bottom element), and search(item). These methods take O(1), O(1), O(1), and O(n) time respectively. Queues are often used in BFS algorithms.</p>

<p><strong>Singly-Linked List:Â </strong>A Singly-Linked List (SLL)Â is an advanced datastructure that is peculiar in the fact that each node is connected only to the next node, with a home and an end node. Meaning that the leastÂ recent addition to the SLL, the Head, is normally the only one directly accessible much like a queue. SSLâ€™sÂ can only be searched by going through each nodeâ€™sÂ â€™nextâ€™ attribute that points to the following node. In a similar way to queue, adding and removing from/to the beginning of a SSL can be done in O(1) time, while adding/removing/searching to any other pointÂ would take O(n) time.</p>

<p><strong>Hash Table:Â </strong>A Hash TableÂ is an advanced datastructure that is peculiar in the fact that itâ€™s essentially an array with a plan. It inputsÂ Â key-value pairs in a way that allows you to access any element in O(1) time and add or delete any item in O(1) time. First, a Hash table is created as a certain size (as are arrays in languages like Java). And uses a particular function e.g. calculateHash(key) that uses the input items key and based off whatever constant algorithm you created in your calculateHashÂ (a popular one is to turn the string into an integer and moduloÂ it by theÂ Â size of the hash table) to create a hashed index that points to an indexÂ position in the array.Â In that index position is your key value pair stored. Hash tables are remarkably efficient (as long as you donâ€™t have too many collisions, something weâ€™ll explain more about later). Hash Tableâ€™s are often considered the gold standard for datastructures to store and hold unsorted user information, and are sometimes used in conjunction with other advanced datastructures like Trees, to be extremely powerful.</p>

<p><strong>Trees:Â </strong>A treeÂ is considered an advanced datastructure, that are similar to SSLâ€™s in the way that theyâ€™re made up of nodes that connect to other nodes. These nodes are called parents when they have nodes under them or their children nodes. Nodes that donâ€™t have children are called leaf nodes. TheÂ firstÂ node is called the root. There are several different sorts of Trees, from trees that only accept 2 children (Binary trees and Binary Search Trees), to some that only offer a single character of information (Tries). Treeâ€™s are used to connect points of information that relate to one another, and can be used with a variety of data structures. Sorted trees often have some of the best Big O search capabilities at O(n log(n)) as they can follow branches based off of simple conditionals.</p>

<h3 id="sorting-algorithms">Sorting Algorithms:</h3>

<p><strong>Quick Sort: Â </strong>The Quick Sort algorithm works by finding a pivot point, and then sorting the two resulting arrays around said pivot point. The act of finding this pivot point will generally sort the array into a higher side, and a lower side. Depending on the size of array, this pivot point will be foundÂ recursively on each minor array. The act of finding this pivot point is an algorithm in itself, and several different of these have made their way into modern use with the Hoare partition being the most efficient. With sorts you do have to worry about the average AND worst case scenario of time it will take, as it can be something to worry about because who knows how your data will be positioned? Itâ€™s average time is O(n log(n)), but it can be as bad as O(n^2) in the right conditions.</p>

<p><a href="https://commons.wikimedia.org/wiki/File:Sorting_quicksort_anim.gif#/media/File:Sorting_quicksort_anim.gif"><img src="https://upload.wikimedia.org/wikipedia/commons/6/6a/Sorting_quicksort_anim.gif" alt="Animated visualization of the quicksort algorithm. The horizontal lines are pivot values." /></a>
By <a href="https://en.wikipedia.org/wiki/User:RolandH">en:User:RolandH</a>, <a href="http://creativecommons.org/licenses/by-sa/3.0/">CC BY-SA 3.0</a>, <a href="https://commons.wikimedia.org/w/index.php?curid=1965827">https://commons.wikimedia.org/w/index.php?curid=1965827</a></p>

<p><strong>Merge Sort:Â </strong>The merge sort works on the â€œDivide and Conquerâ€ theory. It divides lists into sublists, then sorts those sublists, it then combines, and repeats the division asÂ necessary. The final sublist becomes the ultimate sorted list. ItÂ Â is a remarkably resourceful sort whoseÂ expectedÂ and worst case sorts are both O(n log(n)).</p>

<p><a href="https://commons.wikimedia.org/wiki/File:Merge-sort-example-300px.gif#/media/File:Merge-sort-example-300px.gif"><img src="https://upload.wikimedia.org/wikipedia/commons/c/cc/Merge-sort-example-300px.gif" alt="Merge-sort-example-300px.gif" /></a>
By <a href="Swfung8&amp;action=edit&amp;redlink=1">Swfung8</a> - Own work, <a href="http://creativecommons.org/licenses/by-sa/3.0">CC BY-SA 3.0</a>, <a href="https://commons.wikimedia.org/w/index.php?curid=14961648">https://commons.wikimedia.org/w/index.php?curid=14961648</a></p>

<p><strong>Bubble Sort:Â </strong>The bubble sort works by â€œbubbling upâ€ higherÂ values with lower values. Itâ€™s a commonly used sorting tactic for trees and lists, where you can only access one value at a time. In order to do so, you must iterate through the array potentially n^2 times, making it a very time costly sort to do, with an average of O(n^2) time.</p>

<p><a href="https://commons.wikimedia.org/wiki/File:Bubble-sort-example-300px.gif#/media/File:Bubble-sort-example-300px.gif"><img src="https://upload.wikimedia.org/wikipedia/commons/c/c8/Bubble-sort-example-300px.gif" alt="Bubble-sort-example-300px.gif" /></a>
By <a href="Swfung8&amp;action=edit&amp;redlink=1">Swfung8</a> - Own work, <a href="http://creativecommons.org/licenses/by-sa/3.0">CC BY-SA 3.0</a>, <a href="https://commons.wikimedia.org/w/index.php?curid=14953478">https://commons.wikimedia.org/w/index.php?curid=14953478</a></p>

<p><strong>Counting Sort:Â </strong>The counting sort is personally my favorite sort. Why? I think itâ€™s absolutely brilliant. Unfortunately it does require threeÂ bits of knowledge for your array (Iâ€™m sure you could hack around it but for the common implementation these are required)Â that it only has positive values, that each value is an integer, and that you know the max. The graphic below shows it (decently), but it relies on you creating an array from the size of the max value. You then iterate through the firstÂ array, and every time you hit add 1 to the index of the range array to the index of that number. So if you have an array with 3 1â€™s, Â rangeArray[1] will equal 3! This addition to the rangeArray takesÂ O(1) time due to Arrayâ€™s O(1) access time! You then (can) create a third final array which as you go through each index, you check the rangeArray and add each number the corresponding amount of times that it has in its â€œcounterâ€. I think itâ€™s a remarkably brilliant â€œhackâ€ of the arrayâ€™s O(1) access time and can be done consistently at O(n+k) where n are the number of elements and k is itâ€™s range.</p>

<p><img src="https://rubydoobiedoo.files.wordpress.com/2016/09/counting-sort.gif" alt="counting-sort.gif" /></p>

<p>This may seem like a lot to graspâ€¦and youâ€™re right! If youâ€™re new to programming, youâ€™re most likely new to Computer Science principles which all of these are based upon. But I promise, learning these more and more will help you counter any problem you find, and youâ€™ll be amazed at their utility in analytical problem solving in real life and for fun (check outÂ <a href="http://fivethirtyeight.com/tag/the-riddler/">Nate Silverâ€™s company, 538, one of their users posts great riddles one of which I figured out using a Binary search method just the other day!)</a></p>

<p>This was just a brief (lol 2500 words) intro to these concepts, but Iâ€™ll be going into each more in depth in the posts to come! Stay Tuned.</p>
:ET